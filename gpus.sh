# Original training
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_origin --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_origin" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/property_prediction/" --devices "0,1,2,3,4,5,6,7" --filename "ft_property_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 40 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]" --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 1 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 128
python stage2.py --root "data/forward_reaction_prediction/" --devices "0,1,2,3,4,5,6,7" --filename "ft_forward_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --mode "ft" --prompt "[START_I_SMILES]{}[END_I_SMILES]" --tune_gnn --llm_tune "lora" --inference_batch_size 4 --peft_config "lora_config.json" --max_len 600 --batch_size 4 --accumulate_grad_batches 8 --caption_eval_epoch 1


# Cosine attenttion regularization with parameter 1, 8 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_1 --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_1" --stage1_path "all_checkpoints/stage1_cos_1/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_1" --stage2_path "all_checkpoints/stage2_cos_1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 1
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_1" --stage2_path "all_checkpoints/stage2_cos_1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_1" --stage2_path "all_checkpoints/stage2_cos_1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 1


# Cosine attenttion regularization with parameter 0.1, 8 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_01 --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_01" --stage1_path "all_checkpoints/stage1_cos_01/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_01" --stage2_path "all_checkpoints/stage2_cos_01/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.1
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_01" --stage2_path "all_checkpoints/stage2_cos_01/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_01" --stage2_path "all_checkpoints/stage2_cos_01/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.1


# Cosine attenttion regularization with parameter 0.01, 8 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_001 --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_001" --stage1_path "all_checkpoints/stage1_cos_001/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_001" --stage2_path "all_checkpoints/stage2_cos_001/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_001" --stage2_path "all_checkpoints/stage2_cos_001/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_001" --stage2_path "all_checkpoints/stage2_cos_001/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.01


# Cosine attenttion regularization with parameter 0.01, 32 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_001_q32 --rerank_cand_num 128 --num_query_token 32 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_001_q32" --stage1_path "all_checkpoints/stage1_cos_001_q32/last.ckpt" --num_query_token 32 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_001_q32" --stage2_path "all_checkpoints/stage2_cos_001_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01 --accumulate_grad_batches 4 --batch_size 8
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_001_q32" --stage2_path "all_checkpoints/stage2_cos_001_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01 --accumulate_grad_batches 4 --batch_size 8
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_001_q32" --stage2_path "all_checkpoints/stage2_cos_001_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.01 --accumulate_grad_batches 4 --batch_size 8


# Cosine attenttion regularization with parameter 0.01, 2 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_001_q2 --rerank_cand_num 128 --num_query_token 2 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_001_q2" --stage1_path "all_checkpoints/stage1_cos_001_q2/last.ckpt" --num_query_token 2 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_001_q2" --stage2_path "all_checkpoints/stage2_cos_001_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_001_q2" --stage2_path "all_checkpoints/stage2_cos_001_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_001_q2" --stage2_path "all_checkpoints/stage2_cos_001_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.01


# # No pretraining, Only fine-tuning
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_noft" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_noft" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_noft" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 8 --accumulate_grad_batches 4

# # No stage 2 pretraining, fine-tuning after stage 1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 8 --accumulate_grad_batches 4


# Overfitting problem test
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_test_overfit" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 500 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 50 --iupac_prediction --save_every_n_epochs 50
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin_test_overfit" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 500 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 50 --save_every_n_epochs 50
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin_test_overfit" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 500 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 50 --save_every_n_epochs 50


# Overfitting problem test with hard samples
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_hardtest" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10  --save_every_n_epochs 10 --iupac_prediction --use_hards False True True
# python stage2.py --root "data/PubChem324kV2/" --devices "1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_hardtrain" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 1  --save_every_n_epochs 1 --iupac_prediction --use_hards True False False --val_check_interval 0.05


# # Query tests
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_shuffle_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --shuffle_query
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_shuffle_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --shuffle_query
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_shuffle_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --shuffle_query
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_zero_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --zero_query
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_zero_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --zero_query
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_zero_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --zero_query


# # Original train, 32 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_origin_q32 --rerank_cand_num 128 --num_query_token 32 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_origin_q32" --stage1_path "all_checkpoints/stage1_origin_q32/last.ckpt" --num_query_token 32 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --batch_size 4 --accumulate_grad_batches 8
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin_q32" --init_checkpoint "all_checkpoints/stage2_origin_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --accumulate_grad_batches 8 --batch_size 4
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin_q32" --init_checkpoint "all_checkpoints/stage2_origin_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --accumulate_grad_batches 8 --batch_size 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_q32" --init_checkpoint "all_checkpoints/stage2_origin_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --accumulate_grad_batches 8 --batch_size 4


# # Original train, 2 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_origin_q2 --rerank_cand_num 128 --num_query_token 2 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_origin_q2" --stage1_path "all_checkpoints/stage1_origin_q2/last.ckpt" --num_query_token 2 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin_q2" --init_checkpoint "all_checkpoints/stage2_origin_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin_q2" --init_checkpoint "all_checkpoints/stage2_origin_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 8 --accumulate_grad_batches 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_q2" --init_checkpoint "all_checkpoints/stage2_origin_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 8 --accumulate_grad_batches 4


# Query tests, different query indices
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_0_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 0
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_0_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 0
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_0_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 0
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_1_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 1
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_1_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 1
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_1_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 1
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_2_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 2
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_2_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 2
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_2_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 2
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_3_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 3
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_3_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 3
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_3_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 3
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_4_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 4
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_4_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 4
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_4_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 4
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_5_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 5
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_5_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 5
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_5_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 5
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_6_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 6
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_6_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 6
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_6_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 6
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_7_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --max_len 512 --query_index 7
# python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_7_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 7
# python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_index_7_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --max_len 512 --query_index 7
