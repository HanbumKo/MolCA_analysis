# Original training
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_origin --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_origin" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4
python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --save_every_n_epochs 99 --use_hards False False False
python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --save_every_n_epochs 99 --use_hards False False False
python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --save_every_n_epochs 99 --iupac_prediction --use_hards False False False


# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_1 --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_1" --stage1_path "all_checkpoints/stage1_cos_1/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_1" --stage2_path "all_checkpoints/stage2_cos_1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 1
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_1" --stage2_path "all_checkpoints/stage2_cos_1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_1" --stage2_path "all_checkpoints/stage2_cos_1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 1


# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_01 --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_01" --stage1_path "all_checkpoints/stage1_cos_01/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_01" --stage2_path "all_checkpoints/stage2_cos_01/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.1
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_01" --stage2_path "all_checkpoints/stage2_cos_01/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_01" --stage2_path "all_checkpoints/stage2_cos_01/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.1


# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_001 --rerank_cand_num 128 --num_query_token 8 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_001" --stage1_path "all_checkpoints/stage1_cos_001/last.ckpt" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_001" --stage2_path "all_checkpoints/stage2_cos_001/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_001" --stage2_path "all_checkpoints/stage2_cos_001/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_001" --stage2_path "all_checkpoints/stage2_cos_001/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.01


# Cosine attenttion regularization with parameter 0.01, 32 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_001_q32 --rerank_cand_num 128 --num_query_token 32 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_001_q32" --stage1_path "all_checkpoints/stage1_cos_001_q32/last.ckpt" --num_query_token 32 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_001_q32" --stage2_path "all_checkpoints/stage2_cos_001_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01 --accumulate_grad_batches 4 --batch_size 8
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_001_q32" --stage2_path "all_checkpoints/stage2_cos_001_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01 --accumulate_grad_batches 4 --batch_size 8
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_001_q32" --stage2_path "all_checkpoints/stage2_cos_001_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.01 --accumulate_grad_batches 4 --batch_size 8

# Original train, 32 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_origin_q32 --rerank_cand_num 128 --num_query_token 32 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_origin_q32" --stage1_path "all_checkpoints/stage1_origin_q32/last.ckpt" --num_query_token 32 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin_q32" --stage2_path "all_checkpoints/stage2_origin_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --accumulate_grad_batches 4 --batch_size 8
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin_q32" --stage2_path "all_checkpoints/stage2_origin_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --accumulate_grad_batches 4 --batch_size 8
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_q32" --stage2_path "all_checkpoints/stage2_origin_q32/last.ckpt" --num_query_token 32 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --accumulate_grad_batches 4 --batch_size 8

# Original train, 2 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_origin_q2 --rerank_cand_num 128 --num_query_token 2 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_origin_q2" --stage1_path "all_checkpoints/stage1_origin_q2/last.ckpt" --num_query_token 2 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin_q2" --stage2_path "all_checkpoints/stage2_origin_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin_q2" --stage2_path "all_checkpoints/stage2_origin_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_q2" --stage2_path "all_checkpoints/stage2_origin_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction

# Cosine attenttion regularization with parameter 0.01, 2 query tokens
# python stage1.py --root 'data/PubChem324kV2/' --gtm --lm --devices "0,1,2,3,4,5,6,7" --mode train --filename stage1_cos_001_q2 --rerank_cand_num 128 --num_query_token 2 --tune_gnn
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_cos_001_q2" --stage1_path "all_checkpoints/stage1_cos_001_q2/last.ckpt" --num_query_token 2 --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_cos_001_q2" --stage2_path "all_checkpoints/stage2_cos_001_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_cos_001_q2" --stage2_path "all_checkpoints/stage2_cos_001_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --att_reg --att_reg_method cos --att_reg_lambda 0.01
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_cos_001_q2" --stage2_path "all_checkpoints/stage2_cos_001_q2/last.ckpt" --num_query_token 2 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --att_reg --att_reg_method cos --att_reg_lambda 0.01

# No pretraining, Only fine-tuning
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_noft" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_noft" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_noft" --num_query_token 8 --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction

# Overfitting problem test
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_test_overfit" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 500 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 50 --iupac_prediction --save_every_n_epochs 50
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_origin_test_overfit" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 500 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 50 --save_every_n_epochs 50
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_origin_test_overfit" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 500 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 50 --save_every_n_epochs 50

# Overfitting problem test with hard samples
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_hardtest" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10  --save_every_n_epochs 10 --iupac_prediction --use_hards False True True
# python stage2.py --root "data/PubChem324kV2/" --devices "1,2,3,4,5,6,7" --filename "ft_iupac_from_origin_hardtrain" --stage2_path "all_checkpoints/stage2_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 1  --save_every_n_epochs 1 --iupac_prediction --use_hards True False False --val_check_interval 0.05

# Query tests
python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_shuffle_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --use_hards False False False
python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_shuffle_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --use_hards False False False
python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_shuffle_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --use_hards False False False
python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_zero_iupac" --init_checkpoint "all_checkpoints/ft_iupac_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --iupac_prediction --use_hards False False False
python analysis_token_role.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "query_zero_molecule_captioning_pubchem" --init_checkpoint "all_checkpoints/ft_pubchem324k_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --use_hards False False False
python analysis_token_role.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "query_zero_molecule_captioning_chebi" --init_checkpoint "all_checkpoints/ft_chebi20_from_origin/last.ckpt" --mode eval --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --use_hards False False False