# No pretraining, Only fine-tuning
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_noft" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_noft" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_noft" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
python stage2.py --root "data/property_prediction/" --devices "0,1,2,3,4,5,6,7" --filename "ft_property_from_noft" --max_epochs 40 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]" --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 1 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 128


# No stage 2 pretraining, fine-tuning after stage 1
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
python stage2.py --root "data/property_prediction/" --devices "0,1,2,3,4,5,6,7" --filename "ft_property_from_nostage2" --stage1_path "all_checkpoints/stage1_origin/last.ckpt" --max_epochs 40 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]" --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 1 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 128

# No stage 1 pretraining, fine-tuning after stage 2
# python stage2.py --root 'data/PubChem324kV2/' --devices "0,1,2,3,4,5,6,7" --filename "stage2_nostage1" --opt_model 'facebook/galactica-1.3b' --max_epochs 10 --mode pretrain --prompt '[START_I_SMILES]{}[END_I_SMILES].' --tune_gnn --llm_tune freeze --inference_batch_size 4 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_pubchem324k_from_nostage1" --stage2_path "all_checkpoints/stage2_nostage1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/ChEBI-20_data/" --devices "0,1,2,3,4,5,6,7" --filename "ft_chebi20_from_nostage1" --stage2_path "all_checkpoints/stage2_nostage1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]." --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
# python stage2.py --root "data/PubChem324kV2/" --devices "0,1,2,3,4,5,6,7" --filename "ft_iupac_from_nostage1" --stage2_path "all_checkpoints/stage2_nostage1/last.ckpt" --max_epochs 100 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES].The molecule's IUPAC name is " --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 10 --iupac_prediction --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 512
python stage2.py --root "data/property_prediction/" --devices "0,1,2,3,4,5,6,7" --filename "ft_property_from_nostage1" --stage2_path "all_checkpoints/stage2_nostage1/last.ckpt" --max_epochs 40 --mode ft --prompt "[START_I_SMILES]{}[END_I_SMILES]" --tune_gnn --llm_tune lora --inference_batch_size 8 --peft_config lora_config.json --caption_eval_epoch 1 --batch_size 16 --accumulate_grad_batches 2 --text_max_len 128 --max_len 128